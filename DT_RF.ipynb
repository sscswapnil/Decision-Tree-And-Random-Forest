import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

loans = pd.read_csv('~/Desktop/New/Shared/17.11.2019 Decision tree and Random Forest C 5.0/loan_data.csv')

loans.info()

loans.describe()

loans.head()

plt.figure(figsize=(10,6))
loans[loans['credit.policy']==1]['fico'].hist(alpha=0.5,color='blue',
                                              bins=30,label='Credit.Policy=1')
loans[loans['credit.policy']==0]['fico'].hist(alpha=0.5,color='red',
                                              bins=30,label='Credit.Policy=0')
plt.legend()
plt.xlabel('FICO')

plt.figure(figsize=(10,6))
loans[loans['not.fully.paid']==1]['fico'].hist(alpha=0.5,color='blue',
                                              bins=30,label='not.fully.paid=1')
loans[loans['not.fully.paid']==0]['fico'].hist(alpha=0.5,color='red',
                                              bins=30,label='not.fully.paid=0')
plt.legend()
plt.xlabel('FICO')

plt.figure(figsize=(11,7))
sns.countplot(x='purpose',hue='not.fully.paid',data=loans,palette='Set1')

sns.jointplot(x='fico',y='int.rate',data=loans,color='purple')

**trend differed between not.fully.paid and credit.policy.**

plt.figure(figsize=(11,7))
sns.lmplot(y='int.rate',x='fico',data=loans,hue='credit.policy',
           col='not.fully.paid',palette='Set1')
           
# Setting up the Data

loans.info()

## Categorical Features

we need to transform them using dummy variables so sklearn will be able to understand them. we use pd.get_dummies.

cat_feats = ['purpose']

final_data = pd.get_dummies(loans,columns=cat_feats,drop_first=True)
final_data.head()

final_data.info()

## Train Test Split

from sklearn.model_selection import train_test_split

X = final_data.drop('not.fully.paid',axis=1)
y = final_data['not.fully.paid']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)

## Training a Decision Tree Model

from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

dtree = DecisionTreeClassifier()dtree.fit(X_train,y_train)
a=tree.plot_tree(dtree)

## Predictions and Evaluation of Decision Tree

predictions = dtree.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

from sklearn.metrics import accuracy_score,precision_score,recall_score

accuracy_score(y_test,predictions)

precision_score(y_test,predictions)

recall_score(y_test,predictions)

print(classification_report(y_test,predictions))

print(confusion_matrix(y_test,predictions))

## Training the Random Forest model

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=1000)

rfc.fit(X_train,y_train)

## Predictions and Evaluation

predictions = rfc.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

print(classification_report(y_test,predictions))

**Show the Confusion Matrix for the predictions.**

print(confusion_matrix(y_test,predictions))

######## END OF THE ALGORITHM #############
